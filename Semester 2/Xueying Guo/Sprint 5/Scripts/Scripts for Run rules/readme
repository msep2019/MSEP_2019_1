This is repository for saving scripts for running detect system and run rules automatically.

***** meaning for each script ******

----- Run in master node ----
sc1. run_Main.sh ~~ for running BDCA system with generated rules and without applying any rules.
sc2. run_without_properties.sh  ~~ for running BDCA system with default properties only.
sc3. run_with_rules.sh ~~ for running BDCA system with generated rules.
sc4. run_detect_system.sh ~~ for detecting type of datasets and choosing properties by rules. 

sc5. cluster.sh ~~ for collecting utilizations during the BDCA system is running, run before 'run.sh'
sc6. kill_cluster.sh ~~ for stopping collecting utilizations after 'run.sh' is finished.

----- Run in worker nodes ----
sc5. cluster.sh ~~ for collecting utilizations during the BDCA system is running, run before 'run.sh'
sc6. kill_cluster.sh ~~ for stopping collecting utilizations after 'run.sh' is finished.


----- Run in local ----
sc7. get_utilization_evaluation.sh  ~~ for collecting all statistic collected from clusters seperately and calculate average for each node during the training period.
sc8. get_utilization_evaluation_all.sh  ~~ for calculating average statistic for all worker nodes during the training period.
sc9. cleanup.sh  ~~ for cleaning up all temperary files before next batch running.

***** USAGE ******

sc1. ./run_Main.sh 
sc2. ./run_without_properties.sh <algorithm> <dataset> 
 e.g. nohup ./run_without_properties.sh RF CIDD &
sc3. ./run_with_rules.sh <algorithm> <dataset> <parameter command>
 e.g. nohup ./run_with_rules.sh NB CIDD +-D+mapred.child.java.opts=-Xmx1639m+-D+mapreduce.task.io.sort.factor=100+-D+mapreduce.reduce.shuffle.input.buffer.percent=0.8 &
sc4. ./run_detect_system.sh <algorithm> <dataset> <training set>
 e.g. nohup ./run_detect_system.sh NB NSLKDD /user/hadoop/nb/nslkdd/train/NSLKDDTrain.txt &
sc5. ./cluster.sh <number of cluster>
 e.g. nohup ./cluster.sh 1 &
sc6. ./kill_cluster.sh <number of cluster>
 e.g. nohup ./kill_cluster.sh 1 &
sc7. ./get_utilization.sh 
sc8. ./get_utilization_all.sh 
sc9. ./cleanup.sh

***** Batch ******

S1. Run script to collect statistics from master and all worker nodes. [run in master node and worker nodes]
-> nohup ./cluster.sh 1 &

S2. Check parameter files to ensure the required Hadoop properties, datasets and machine learning algorithms can be run by the BDCA system.
-> test_algorithm.param
-> rules.param
-> all_conf.param
-> algorithm.param
-> package_name.param

S3. Run BDCA system by script. [run in master node only]
->  nohup ./run_Main.sh &

S4. Check logs and make sure S3 is completed.

S5. Run script to stop collecting statistics from master and all worker nodes. [run in master node and worker nodes]
-> nohup ./kill_cluster.sh 1 &

S6. Check parameter files to ensure all statistics got by required datasets and ML algorithms can be calculated.
-> cal_algorithm.param

S7. Run script to collecting statistics from cluster and calculating utilization. [run in local]
-> nohup ./get_utilization_evaluation.sh &

S8. Run script to calculating average utilization for all worker nodes. [run in local]
-> nohup ./get_utilization_evaluation_all.sh &

S9. Run script to clean up temperary files produced during the batch running.
-> nohup ./cleanup.sh &

S10. check result files.


close issue #134
