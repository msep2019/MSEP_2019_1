PETS consists of 4 parts:
1. controller -> metrics collection from log, d-stat, spark internal metrics
2. metrics processor -> calculate runtime and bottleneck score (CPU, memory, disk, and network)
3. Fuzzy logic component-> to determine how to change the parameter by mapping a truth table.
4. parameter ensemble table -> to determine the value of the parameter that should be applied. This ensemble table groups all possible parameters together and assigns 2 directions (increase or decrease value) to different parameters so that determine a value for each parameter according to the bottleneck score. 

By doing this, the system can tune all parameters (18 in the paper) at once.

For Hadoop, I believe we can have a similar idea:
1. controller -> Run job (by default or changed parameter) and metrics collection from log, d-stat 
2. metrics processor -> get runtime information and quantify bottleneck score (CPU, memory, disk, net, i/o, block io in our case). 
3. parameter table -> combine 3&4 from 'PETS', according to the score calculated from step 2, we can know the direction of the parameter, determine what should be the value in order to optimize the performance according to the parameter table.

